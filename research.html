<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Fariha Iffath</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="icon" type="image/png" href="images/cloud.png" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo">Fariha Iffath</a>
									<ul class="icons">
										<li><a href="https://scholar.google.com/citations?user=93H417MAAAAJ&hl=en" class="icon fa-google">
											<span class="label">Google Scholar</span></a></li>
										<li><a href="https://www.linkedin.com/in/fariha-iffath/" class="icon fa-linkedin">
											<span class="label">LinkedIn</span></a></li>
										<li><a href="https://github.com/settings/profile/Fariha Iffath" class="icon fa-github">
											<span class="label">GitHub</span></a></li>
										<!--<li><a href="https://twitter.com/ClareESinger" class="icon fa-twitter">
											<span class="label">Twitter</span></a></li> -->
										<li><a href="https://orcid.org/my-orcid?orcid=0000-0003-4292-4335" class="logo">
											<span class="label">ORCiD</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>Research</h1>
									</header>
                                      
									<hr class="major" />
									
									<h3>ARF-Net: a multi-modal aesthetic attention-based fusion <a href="https://link.springer.com/article/10.1007/s00371-024-03492-2"><u>(Visit Research)</u></a></h3>
									<p> <em>Can a deep fusion method be established for biometric identification?</em> 
										<ul>
											<li>Over the last decade, Online Social Media platforms have witnessed a dramatic expansion due to the substantial reliance of
individuals on these communication channels. These platforms are widely utilized to convey emotions, share opinions, and
express preferences through various means such as artworks, multimedia contents, and blogs. Researchers are exploring these
individual-specific traits for biometric identification. Aesthetic biometric systems utilize usersâ€™ unique preferences across
various subjective forms such as images, music, and textual contents. This study introduces a novel multi-modal aesthetic
system, with a primary contribution to the development of an attention-based fusion method for person identification. The
proposed identification system leverages a deep pre-trained model for high-level feature extraction from visual and auditory
modalities. The paper introduces a novel fusion architecture named attention-based residual fusion network (ARF-Net) to
incorporate two heterogeneous aesthetic feature vectors. The proposed model yielded a 99.38% identification accuracy on
the Aesthetic Image Audio 32 (AIA32) dataset and 98.02% identification accuracy on Aesthetic Image Audio 52 (AIA52)
dataset, outperforming other aesthetic biometric systems. The proposed architecture stands out for its efficiency, showcasing
a lightweight architecture with minimal parameters, ensuring optimal performance in different modalities. 
</li>
                                            
										  </ul>
										  <center>
								    <iframe src="files/overall_block.pdf" height="250" width="600"></iframe><br>
										Fig 1. Architecture of the proposed multi-modal aesthetic system.<br>
									
										</center>
										</p>

										 

									
									<h3>Unimodal Aesthetic Biometric System <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=93H417MAAAAJ&sortby=pubdate&citation_for_view=93H417MAAAAJ:qjMakFHDy7sC"><u>(Visit Research)</u></a></h3>
									
									<p> <em>Can a deep learning-based aesthetic system can identify users with the highest precision and lower inference time</em> 
									    <ul>
											<li> A novel three-stage framework based on deep learning and classical machine learning is proposed for 
											identifying individuals from their audio aesthetic. To extract audio features, mel-spectrograms are used instead of generic spectrograms. 
											As a result, the extracted feature set becomes more discriminating for person identification.
											A novel hybrid meta-heuristic algorithm Cuckoo Search based Whale Optimization Algorithm (CSWOA) 
											is proposed that retrieves the most optimal feature subset from the high-level features.</li>
										</ul>
										<center>
									
									<iframe src="files\Feature_extraction_v2_audio.pdf" height="300" width="640"></iframe><br>
										Fig 4. Proposed Hybrid meta-heuristic feature selection algorithm (CSWOA) along with transfer learning-based feature extraction method.		
											</center>
									<!--<p>Read the paper here: <a href="https://ieeexplore.ieee.org/document/9862985?source=authoralert">
										https://doi.org/10.1109/ACCESS.2022.3200166</a></p>

										<!--<p><iframe src="files\Hybrid_Flowchart.drawio (1)" height="400" width="320" id="fig1"></iframe>
											
											Fig 1. Top-of-atmosphere flux (a) and albedo (b) bias as a function of solar 
											zenith angle for various cloud types: shallow cumulus (BOMEX and RICO), 
											stratocumulus (DYCOMS-II RF01), deep convection (TRMM-LBA and TRMM-LBA agg.).
										</p>
										<p style="clear: left;"></p>
										
										<p><iframe src="files/fig-bias-cwp-map.pdf" height="200" width="600" id="fig2"></iframe>
											
											Fig 2. Zonal-mean and map of annual-mean flux bias inferred from ISCCP cloud water path. 
											Bias is smallest over stratocumulus regions and largest over the ITCZ and storm tracks,
											areas with deep clouds and persistent cloudiness.
										</p>
										<p style="clear: left;"></p> -->
										
										<!--<em>In collaboration with: <a href="https://pages.cpsc.ucalgary.ca/~marina/HomePHP/index.php">Dr. Marina Gavrilova</a>, -->
											 
									</p>

									
									<h3>Online Judging Platform Utilizing Dynamic Plagiarism Detection Facilities <a href="https://www.mdpi.com/2073-431X/10/4/47"><u>(Visit Research)</u></a></h3>
									<p> 
										<em>What approaches can be implemented to develop an efficient plagiarism detection system for programming problems 
										in an online judging platform, considering the challenges faced by teachers in identifying source code plagiarism
										and the prevalence of plagiarism among students?</em> 
										<ul>
								
									        <li>Designed an online judging framework designed specifically for programming labs, 
											addressing the limitations of traditional online judging platforms. 
											The proposed system incorporates automatic scoring of codes with efficient 
											detection of plagiarized content, utilizing program fingerprints generated 
											by the Rabin-Karp Algorithm. By selecting fingerprints through winnowing among k-gram hash values,
											the system improves time efficiency, correctness, and 
											feature availability compared to existing online judging platforms. 
											The evaluation of the system with large datasets and comparison 
											of runtime with the widely used MOSS plagiarism detection technique further demonstrates its superiority.</li>
										</ul>
										<center>
										<p><iframe src="files\online judge.pdf" height="650" width="640"></iframe><br>
										Fig 5. Flowchart of the proposed Online Judging System.<br>
										
										</p>
										</center>
										
									<h3>Human Activity Recognition <a href="https://link.springer.com/chapter/10.1007/978-981-16-8558-3_12"><u>(Visit Research)</u></a></h3>
									<p> 
										<em>Can the integration of transfer learning with a two-stream neural network architecture, 
										utilizing 3D dense optical flow, result in a significantly improved Human Activity Recognition (HAR) system? </em> 
										<ul>
								
									        <li>In this work a novel human activity recognition technique is developed that combines action recognition 
											 and 3D dense optical flow from video sequences, leveraging the efficiency of optical flow as a feature for accurate recognition.
											and 3D dense optical flow from video sequences, leveraging the efficiency of optical flow as a feature for 
											accurate recognition. The proposed method utilizes transfer learning with a two-stream neural network architecture, 
											incorporating the pre-trained ResNet152 architecture, to extract fine-grained features from the dense optical flow. 
											The proposed approach was evaluated on the UCF-101 dataset, highlighting its potential for advancing human activity
											recognition in various computer vision applications.</li>
											
										</ul>
										<center>
										<p><iframe src="files\HAR_FlochartDiagram.pdf" height="100" width="640"></iframe><br>
										Fig 4. Architecture of the proposed Human Activity Recognition(HAR) system.<br>
										</p>
										</center>
										
									<h3>Cervical Cancer Prediction <a href="https://link.springer.com/chapter/10.1007/978-981-16-6636-0_2"><u>(Visit Research)</u></a></h3>
									<p> 
										<em>Can machine learning algorithms along with Adaptive Synthetic Sampling(ADASYN) and 
										Linear Discriminant Analysis (LDA) identify patterns or interactions among risk factors that may enhance 
										the prediction of cervical cancer, and how do these findings align with existing knowledge in the field?</em> 
										<ul>
								
									        <li>In this study, a significant contributions were made in developing a cervical cancer prediction system.
											Missing value imputation led to higher precision, highlighting the importance of addressing missing data in 
											cervical cancer datasets. Additionally,  Linear Discriminant Analysis (LDA) is implemented for dimensionality 
											reduction and the Adaptive Synthetic Sampling approach (ADASYN) is adopted to balance the dataset, resulting in
											improved outcomes. Moreover, the Isolation Forest algorithm is used, to identify and removed outliers in the 
											cervical cancer datasets, enhancing the quality of the data. Lastly, an effective classifier model using various 
											machine learning techniques  is developed and evaluated their performence using different evaluation approaches, 
											resulting in higher performance efficacy for cervical cancer prediction.</li>
											
										</ul>
										<center>
										<p><iframe src="files\final_methodology_cervical.pdf" height="160" width="640"></iframe><br>
										Fig 5. Architecture of the proposed Cervical Cancer Prediction system. <br>
										</p>
										</center>
										
									</p>
									

								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
								</form>
								</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Homepage</a></li>
										<li><a href="research.html">Research</a></li>
										<li><a href="teaching.html">Teaching/Mentoring</a></li>
										<li><a href="projects.html">Projects</a></li>
										<li><a href="outreach.html">Outreach/Leadership</a></li>
										<li><a href="publications.html">Publications</a></li>
										<li><a href="cv.html">CV</a></li>
										
								</nav>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<p>I'm always happy to chat about my research!</p>
									<ul class="contact">
										<li class="fa-envelope-o"><a href="#">fariha.iffath@ucalgary.ca</a></li>
										<li class="fa-home">2903 Unwin Road, Calgary<br />
									
									</ul>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Clare Singer. All rights reserved. Design: 
										<a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
